---
title: "Text as Data HW 1"
author: "Chenjie Su -- cs5998"
date: "3/04/2020"
output: pdf_document
--- 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls())
# import libraries
library(quanteda)
library(quanteda.corpora)
library(gutenbergr)
library(dplyr)
library(stylest)
library(corpus)
library(sophistication)
library(pbapply)
```

## Q1a

```{r}
inaugural <- data_corpus_inaugural
#the inaugural addresses given by Richard Nixon in 1969
rn1969 <- corpus_subset(inaugural, Year=='1969') #get the corpus subset
rn1969T <- texts(rn1969) #get the texts
rn1969tokens <- tokens(rn1969T, remove_punct = TRUE) #tokenizing
TTR1969 <- textstat_lexdiv(rn1969tokens, measure = "TTR") #get the TTR
TTR1969
```
```{r}
#the inaugural addresses given by Richard Nixon in 1973
rn1973 <- corpus_subset(inaugural, Year=='1973')
rn1973T <- texts(rn1973)
rn1973tokens <- tokens(rn1973T, remove_punct = TRUE) 
TTR1973 <- textstat_lexdiv(rn1973tokens, measure = "TTR")
TTR1973
```
**For the inaugural addresses (of president Richard Nixon) in 1969, TTR=0.3333333, in 1973, TTR=0.2815103. **

## Q1b

```{r}
# create dfm, remove the punctuation without other prepossesing 
n1969_n1973_dfm <- dfm(c(rn1969T, rn1973T), remove_punct = TRUE)
n1969_n1973_dfm[,1:6]
# Calculate similarity
similarity_nixon1969_nixon1973 <- textstat_simil(n1969_n1973_dfm, 
                                                 margin = "documents", 
                                                 method = "cosine")
as.matrix(similarity_nixon1969_nixon1973)
```
**In this question, we need to create a document feature matrix of the two speeches. And calculate the cosine similarity which is 0.9223596.**

## Q2a

```{r echo=TRUE}
# stemming the words
stemmed_1969 <- tokens_wordstem(rn1969tokens)
stemmed_1973 <- tokens_wordstem(rn1973tokens)
# redo 1a
TTR1969a <- textstat_lexdiv(stemmed_1969, measure = "TTR")
TTR1973a <- textstat_lexdiv(stemmed_1973, measure = "TTR")
TTR1969a
TTR1973a
# redo 1b
n1969_n1973_dfm_2a <- dfm(c(stemmed_1969, stemmed_1973))
similarity_nixon1969_nixon1973_2a <- textstat_simil(n1969_n1973_dfm_2a, 
                                                    margin = "documents", 
                                                    method = "cosine")
as.matrix(similarity_nixon1969_nixon1973_2a)
```
*Q2a: Results: TTR decrease, similarity decrease.*

**After stemming the words, the TTR will decrease. TTR = total types/total tokens, when we stem the words, the tokens keep the same, however the types get smaller since types are the sets of unique tokens. The tokens who have the same root will become one type. So TTR get smaller.**

**And for the similarity, cosine similarity captures the style or topic in document. When we stem the words, if their styles are different, there will be less common words in two documents. The direction of the documents is getting further, their difference become larger.**

## Q2b

```{r}
# removing stop words
nostop_1969 <- tokens_select(rn1969tokens, pattern = stopwords('en'), selection = 'remove')
nostop_1973 <- tokens_select(rn1973tokens, pattern = stopwords('en'), selection = 'remove')
# redo 1a
TTR1969b <- textstat_lexdiv(nostop_1969, measure = "TTR")
TTR1973b<- textstat_lexdiv(nostop_1973, measure = "TTR")
print(TTR1969b)
print(TTR1973b)
#redo 1b
n1969_n1973_dfm_2b <- dfm(c(nostop_1969, nostop_1973))
similarity_nixon1969_nixon1973_2b <- textstat_simil(n1969_n1973_dfm_2b, 
                                                    margin = "documents", 
                                                    method = "cosine")
as.matrix(similarity_nixon1969_nixon1973_2b)
```
*Q2b: Results: TTR increase, similarity decrease.*

**After removing stop words, TTR will rise up. The tokens decreased, types are decreasing too but not that much. Because the major words still exist, those unique sets(types) keep the same. So the type-token ratio increased. For the similarity: generally, stop words are very common in the documents, when we remove the stop words, the difference between two documents is larger than before. So the cosine similarity is getting smaller.**

## Q2c

```{r}
# Converting all words to lowercase
lowercase_1969 <- tokens_tolower(rn1969tokens)
lowercase_1973 <- tokens_tolower(rn1973tokens)
# redo 1a
TTR1969c <- textstat_lexdiv(lowercase_1969, measure = "TTR")
TTR1973c<- textstat_lexdiv(lowercase_1973, measure = "TTR")
print(TTR1969c)
print(TTR1973c)
#redo 1b
n1969_n1973_dfm_2c <- dfm(c(lowercase_1969, lowercase_1973))
similarity_nixon1969_nixon1973_2c <- textstat_simil(n1969_n1973_dfm_2c, 
                                                    margin = "documents", 
                                                    method = "cosine")
as.matrix(similarity_nixon1969_nixon1973_2c)
```
*Q2c: Results: TTR and similarity keep the same. *

**After converting all words to lowercase, TTR and similarity will not change. Probably because there are no special uppercase words in two documents. So lowercase the words won't change the type-token ratio and similarity between two documents. But there might be some words like the first word of the sentence -> lowercase, less types and more tokens, TTR will increase a little bit.**

## Q2d

```{r}
# use tfidf based on the whole corpus
full_dfm <- dfm(inaugural, remove_punct = TRUE)
topfeatures(full_dfm)
# Frequency weighting 
weighted_dfm <- dfm_tfidf(full_dfm) # uses the absolute frequency of terms in each document
topfeatures(weighted_dfm)
# Relative frequency weighting 
proportional <- dfm_weight(full_dfm, scheme = "prop")
normalized <- dfm_tfidf(full_dfm, scheme_tf = "prop") 
topfeatures(normalized)
```
**Q2b: Results: TTR and similarity keep the same.**
**I just used the codes from the lab recitation to analyze the whole corpus. There is no prepossessing based on the documents, so the TTR and similarity won't change. The tf-idf is term frequency-inverse document frequency. I think tf-idf is just a way to demonstrate if a given word is indicative of difference in the documents. In this question, we only have two documents. Tfidf does not make sense in this question.**  

## Q3

```{r}
t1 <- texts("Trump Says He’s ‘Not Happy’ With Border Deal, but Doesn’t Say if He Will Sign It.")
t2 <- texts("Trump ‘not happy’ with border deal, weighing options for building wall.")
t1dfm <- dfm(t1,tolower=T,remove_punct = TRUE)
t2dfm <- dfm(t2,tolower=T,remove_punct = TRUE)
# covert to vectors
zeros = c(0,0,0,0,0)
v1 = as.vector(t1dfm)
v2 = append(as.vector(t2dfm),zeros)
```
**In pre-processing, I only choose to cover the words to lowercase since there are many capital words in the first sentence which are not important. Some negative words are very important for understanding the sentence. So we can't simply stem the words. Also, stop words like 'doesn't' are essential, so I choose to keep the stop words.**

## Q3a,b,c--do calculate

```{r}
calculate_distance <- function(vec1, vec2) { 
  euc_dist = sqrt(sum((vec1 - vec2)^2))
  man_dist = sum(abs(vec1 - vec2))
  nominator <- vec1%*% vec2  
  denominator <- sqrt(vec1 %*% vec1)*sqrt(vec2 %*% vec2)
  cos_dist = nominator/denominator
  return(list(Euclidean = euc_dist, Manhattan = man_dist,Cosine=cos_dist))
}
calculate_distance(v1,v2)
```
**The Euclidean  distance of the two sentences is 2.236068, the Manhattan distance between them is 5, the cosine similarity between them is 0.8291562.**

## Q4a & Q4b

Get the data (download the novels) and create a data frame.

```{r}
#get the gutenberg_id which we need
gbid <- c() 
for (i in c("Austen, Jane","Dickens, Charles","Alcott, Louisa May","Brontë, Charlotte")){
   gid = gutenberg_works(author == i)[1:4,1] #id from 1-4 rows, the first column
   gbid = rbind(gbid,gid)
}
gbid

# covert gbid, remove the name of column
book= c()
for (i in gbid){
  book <- rbind(book,i)
}

#download books, extract 500 lines of each text, create the dataframe
books = data.frame()
for (b in book){
  book_ <- gutenberg_download(b,meta_fields = c('title','author'))
  lines <- book_[100:600,]
  books <- rbind(books,lines)
}

# remove the special characters that is non-utf-8
df <- data.frame(apply(books, 2, function(x) {x <- gsub("\xa0", "", x)}))
str(df)
```
```{r}
df[1:2,]
```

## Q4c

Use the stylest select vocab function to select the terms in my model.

```{r}
filter <- corpus::text_filter(drop_punct = TRUE, drop_number = TRUE, drop=stopwords_en)
set.seed(2020L)
# fits n-fold cross-validation, I choose n=10
vocab_custom <- stylest_select_vocab(df$text, df$author,  
                                     filter = filter, smooth = 1, nfold = 10, 
                                     cutoff_pcts = c(10, 20, 30, 40, 50, 60, 70, 80, 90))
# percentile with best prediction rate, the best speaker classification rate
vocab_custom$cutoff_pct_best 
# rate of incorrectly predicted speakers of held-out texts, 
#matrix of the mean percentage of incorrectly identified speakers for each cutoff percent and fold
vocab_custom$miss_pct 
```
**For the pre-processing, I choose to drop punctuation which is quite normal. And drop number, I think numeric characters are not important in literature works. Plus, remove the stop words since there are so many irrelevant stop words in novels.**

**So, 10 percentile of term frequency has the best prediction rate. And the above shows the matrix of the mean percentage of incorrectly identified speakers for each cutoff percent and fold**

## Q4d

```{r}
# subset features # USE SAME FILTER, get the terms
vocab_subset <- stylest_terms(df$text,df$author, vocab_custom$cutoff_pct_best , filter = filter) 
# fit the model
style_model <- stylest_fit(df$text,df$author, terms = vocab_subset, filter = filter)
# explore output # influential terms, compute the influence of the terms
head(stylest_term_influence(style_model, df$text, df$author))  
# report the top 5 terms
authors <- unique(df$author)
term_usage <- style_model$rate
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,][1:5])])) %>% setNames(authors)
```
**I think these words don't make sense.**

## Q4e

```{r}
# choose: "Austen, Jane","Dickens, Charles"
AJ = term_usage[1,]
DC = term_usage[2,]
sort(AJ,decreasing=TRUE)[1:5]
sort(DC,decreasing=TRUE)[1:5]
```
**I think the ratio shows the words that the specific author would like to use in his/her novel. For instance, Dickens, Charles used the word "mr" more often than Austen, Jane.**

## Q4f

```{r}
mystery_ex = readRDS("/Users/chenjiesu/Desktop/Text_as_Data_Assignments/mystery_excerpt.rds")
pred = stylest_predict(style_model,mystery_ex)
pred$predicted
```
**According to the model, Bronte Charlotte is the most likely author to the mystery excerpt.**

## Q4g

**From the results below, I think the 10 collocations with the largest lambda value can perform better to be multi-word expressions (lambda is the n-way interaction term from a saturated log-linear model).**
```{r}
# bigrams
g1<-toString(df[1:500,2])#Austen, Jane: Persuasion
textstat_collocations(g1, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g1, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g2<-toString(df[501:1000,2])#Austen, Jane: Northanger Abbey
textstat_collocations(g2, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g2, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g3<-toString(df[1001:1500,2])#Austen, Jane: Mansfield Park
textstat_collocations(g3, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g3, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g4<-toString(df[1501:2000,2])#Austen, Jane: Emma
textstat_collocations(g4, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g4, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
#Dickens, Charles: A Christmas Carol in Prose; Being a Ghost Story of Christmas
g5<-toString(df[2001:2500,2])
textstat_collocations(g5, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g5, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g6<-toString(df[2501:3000,2])#Dickens, Charles: A Tale of Two Cities
textstat_collocations(g6, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g6, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g7<-toString(df[3001:3500,2])#Dickens, Charles: The Mystery of Edwin Drood
textstat_collocations(g7, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g7, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g8<-toString(df[3501:4000,2])#Dickens, Charles: The Pickwick Papers
textstat_collocations(g8, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g8, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g9<-toString(df[4001:4500,2])#Alcott, Louisa May: Flower Fables
textstat_collocations(g9, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g9, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g10<-toString(df[4501:5000,2])#Alcott, Louisa May: Little Women
textstat_collocations(g10, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g10, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g11<-toString(df[5001:5500,2])#Alcott, Louisa May: Eight Cousins
textstat_collocations(g11, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g11, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g12<-toString(df[5501:6000,2])#Alcott, Louisa May: Jack and Jill
textstat_collocations(g12, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g12, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g13<-toString(df[6001:6500,2])#Brontë, Charlotte: The Professor
textstat_collocations(g13, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g13, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g14<-toString(df[6501:7000,2])#Brontë, Charlotte: Jane Eyre: An Autobiography
textstat_collocations(g14, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g14, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g15<-toString(df[7001:7500,2])#Brontë, Charlotte: Villette
textstat_collocations(g15, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g15, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```


```{r}
g16<-toString(df[7501:8000,2])#Brontë, Charlotte: Shirley
textstat_collocations(g16, size = 2, min_count = 5) %>% arrange(-lambda) %>% slice(1:10)
textstat_collocations(g16, size = 2, min_count = 5) %>% arrange(-count) %>% slice(1:10)
```

## Q5a

```{r}
# make the snippets of one sentence, between 150-350 chars in length
# data(data_corpus_ukmanifestos, package = "quanteda.corpora")
snippetData <- snippets_make(data_corpus_ukmanifestos, nsentence = 1, minchar = 150, maxchar = 350)
# clean up the snippets and get the top 10
snippetData <- snippets_clean(snippetData)
head(snippetData,10)
```

## Q5b

```{r}
set.seed(2020)
# generate more pairs from a larger sample of data
snippetPairsAll <- pairs_regular_make(snippetData[sample(1:nrow(snippetData), 1000), ])
# Make some "Gold" questions -- for use with CrowdFlower workers 
gold_questions <- pairs_gold_make(snippetPairsAll, n.pairs = 10)
gold_questions
```
*pairs_regular_make: create test questions for CrowdFlower from sample of snippet pairs, where the readability is the most different between pairs.*

My classification: **B, A, A, B, A, B, B, A, A, B**

Machine Classific: **B, A, A, B, A, B, A, A, A, B**

**So, 90% of the ten gold pairs we reach the agreement. For the number 7 pairs, there is an url link in the text B, so I think the text A is easier to understand. But machine might think there are more common words in text B.**

## Q6 

```{r}
# download the texts
T1232 = gutenberg_download(1232)$text
L3207= gutenberg_download(3207)$text
# Zipf's law as a feature selection tool
dfm1232<- dfm(T1232, tolower=TRUE, remove_punct = TRUE, remove=stopwords("english")) # pre-processing
plot(log10(1:100), log10(topfeatures(dfm1232, 100)),
     xlab = "log10(rank)", ylab = "log10(frequency)", main = "Zipf's Law",col = "red")
# Fits a linear regression
regression <- lm(log10(topfeatures(dfm1232, 100)) ~ log10(1:100))
abline(regression, col = "red")
confint(regression)
summary(regression)
# for leviathan
dfm3207 <- dfm(L3207, tolower=TRUE,remove_punct = TRUE,remove=stopwords("english"))
points(log10(1:100), log10(topfeatures(dfm3207, 100)),
     xlab = "log10(rank)", ylab = "log10(frequency)", main = "Zipf's Law",col="blue")
# regression
regression_ <- lm(log10(topfeatures(dfm3207, 100)) ~ log10(1:100))
abline(regression_, col = "blue")
confint(regression_)
summary(regression_)
```
**For the pre-processing, I converted all the words to lowercase, removed punctuation and stop words. From the graph, we can know the difference between two novels. And also, the graph of log(corpus frequency) function is almost linear which validate the zipf's law.**

## Q7

```{r}
k <- 44
M1232<- nfeat(dfm1232) # M: total number of types(features)
M3207<- nfeat(dfm3207) 
TT1232<- sum(ntoken(dfm1232)) # T: get the number of tokens (the text size) 
TT3207<- sum(ntoken(dfm3207))
#caculate b
# M = k*(T)^b => logM = b*log(k*(T)) => b = logM / log(k*(T))
b1232 <- log(M1232/k,TT1232)
b3207 <- log(M3207/k,TT3207)

#report the result
print (paste0("The value of b in Machiavelli’s “The Prince” is ",b1232))
print (paste0("The value of b in Hobbes’s “Leviathan” is ",b3207))
```
As I used the dfm from Q6, I did the same preprocessing as Q6. The value of b in Machiavelli’s “The Prince” is 0.475578684701218. And the value of b in Hobbes’s “Leviathan” is 0.47207615642333.

## Q8

```{r}
# use kwic() to get the info of key words
# choose politic*, environment, events, administration, government, public, affair, power
#all the words are political words first come to my mind .
kw_1232 <- kwic(T1232, pattern = c('politic*','environment', 'events', 'administration', 
                                   'government', 'public', 'affair','power'), window = 7)
kw_3207 <- kwic(L3207, pattern = c('politic*','environment', 'events', 'administration', 
                                   'government', 'public', 'affair', 'power'), window = 7)
head(kw_1232,10)
head(kw_3207,10)
nrow(kw_1232)
nrow(kw_3207)
```
**For the words I selected, there are 958 rows of data about those words in The Prince, only 86 rows in Leviathan. In both works, major are 'power' and 'government'. That's very interesting.**
```{r}
# how about war related words?
kw_1232_1 <- kwic(T1232, pattern = c('princedom','kingdom', 'conquer','fortune',
                                     'arms','military','war'), window = 7)
kw_3207_1 <- kwic(L3207, pattern = c('princedom','kingdom', 'conquer','fortune',
                                     'arms','military','war'), window = 7)
nrow(kw_1232_1)
nrow(kw_3207_1)
```
**I briefly choose the words from The Prince, which I think those might be more common in the book. There are 219 rows in The Prince and 105 rows in Leviathan. So I guess there might be more contents about war in The Prince.**
```{r}
# words about religion
kw_1232_2 <- kwic(T1232, pattern = c('wealth', 'right','christ', 'authority',
                                     'right', 'church', 'religon*'), window = 7)
kw_3207_2 <- kwic(L3207, pattern = c('wealth', 'right','christ', 'authority',
                                     'right', 'church', 'religon*'), window = 7)
nrow(kw_1232_2)
nrow(kw_3207_2)
```
**I selected the words about religions. From the size of key words data frame, we can know there are much more information about religions in Leviathan.**

## Q9a

```{r}
head(docvars(data_corpus_ukmanifestos), 10)
```

```{r}
CorpSub <- corpus_subset(data_corpus_ukmanifestos, Party=='Lab')
sentences = corpus_reshape(CorpSub, to = "sentences")
head(docvars(CorpSub), 10) # see the selected data
# convert corpus to df 
uklabdf <- sentences$documents %>% select(texts, Party, Year) %>% mutate(Year = as.integer(Year))
# Let's filter out any NAs
uklabdf <- na.omit(uklabdf)
# mean Flesch statistic per year
flesch_point <- uklabdf$texts %>% textstat_readability(measure = "Flesch") %>% 
  group_by(uklabdf$Year) %>% 
  summarise(mean_flesch = mean(Flesch)) %>% 
  setNames(c("Year", "mean")) %>% arrange(Year)
flesch_point
```


```{r}
# We will use a loop to bootstrap a sample of texts and subsequently calculate standard errors
iters <- 10

library(pbapply)
Years = unique(uklabdf$Year)
# build function to be used in bootstrapping
boot_flesch <- function(Year_data){
  N <- nrow(Year_data)
  bootstrap_sample <- sample_n(Year_data, N, replace = TRUE)
  readability_results <- textstat_readability(bootstrap_sample$texts, measure = "Flesch")
  return(mean(readability_results$Flesch))
}

# apply function to each year
boot_flesch_by_Year <- pblapply(Years, function(x){
  sub_data <- uklabdf %>% filter(Year == x)
  output_flesch <- lapply(1:iters, function(i) boot_flesch(sub_data))
  return(unlist(output_flesch))
})
names(boot_flesch_by_Year) <- Years

# compute mean and std.errors
Year_means <- lapply(boot_flesch_by_Year, mean) %>% unname() %>% unlist()
Year_ses <- lapply(boot_flesch_by_Year, sd) %>% unname() %>% unlist() 
# bootstrap standard error = sample standard deviation bootstrap distribution
Year_ses
```
```{r}
#report in a table
cbind(flesch_point,Year_ses)
```

## 9b

```{r}
# FRE
readability <- textstat_readability(CorpSub, "Flesch") #%>% head()
readability
# textstat_readability(texts(CorpSub, groups=="Year"), "Flesch"), there is no comparison in CorbSub
# but we can get the FRE in the whole corpus by using: 
# textstat_readability(texts(data_corpus_ukmanifestos, groups = "Year"), "Flesch")
```

```{r}
cbind(flesch_point,readability)
```
**Bootstapping is via random sampling with replacement from our sample. Form the table above, I think the values are very similar.**